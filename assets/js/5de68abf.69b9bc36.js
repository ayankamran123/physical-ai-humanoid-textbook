"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[476],{3658:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-07","title":"Chapter 7","description":"Chapter 07: Vision & Perception","source":"@site/docs/chapter-07.md","sourceDirName":".","slug":"/chapter-07","permalink":"/physical-ai-humanoid-textbook/docs/chapter-07","draft":false,"unlisted":false,"editUrl":"https://github.com/ayankamran123/physical-ai-humanoid-textbook/tree/main/docs/chapter-07.md","tags":[],"version":"current","frontMatter":{"id":"chapter-07","title":"Chapter 7","sidebar_label":"Chapter 7"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6","permalink":"/physical-ai-humanoid-textbook/docs/chapter-06"},"next":{"title":"Chapter 8","permalink":"/physical-ai-humanoid-textbook/docs/chapter-08"}}');var t=i(4848),o=i(8453);const a={id:"chapter-07",title:"Chapter 7",sidebar_label:"Chapter 7"},r=void 0,l={},c=[{value:"Chapter 07: Vision &amp; Perception",id:"chapter-07-vision--perception",level:3},{value:"Learning Objectives",id:"learning-objectives",level:4},{value:"Theory Explanation",id:"theory-explanation",level:4},{value:"Diagrams",id:"diagrams",level:4},{value:"Python/ROS2 Code Examples",id:"pythonros2-code-examples",level:4},{value:"Python with OpenCV: Edge Detection",id:"python-with-opencv-edge-detection",level:5},{value:"ROS2: Conceptual Vision Node (Pseudocode)",id:"ros2-conceptual-vision-node-pseudocode",level:5},{value:"Exercises + MCQs",id:"exercises--mcqs",level:4},{value:"Exercises",id:"exercises",level:5},{value:"Multiple Choice Questions",id:"multiple-choice-questions",level:5}];function d(e){const n={admonition:"admonition",code:"code",em:"em",h3:"h3",h4:"h4",h5:"h5",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h3,{id:"chapter-07-vision--perception",children:"Chapter 07: Vision & Perception"}),"\n",(0,t.jsx)(n.h4,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After studying this chapter, you should be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the basics of camera operation and calibration."}),"\n",(0,t.jsx)(n.li,{children:"Apply fundamental computer vision algorithms for image processing."}),"\n",(0,t.jsx)(n.li,{children:"Describe principles of object detection and recognition."}),"\n",(0,t.jsx)(n.li,{children:"Explain the concepts of Simultaneous Localization and Mapping (SLAM)."}),"\n",(0,t.jsx)(n.li,{children:"Implement simple vision tasks using Python with OpenCV and integrate with ROS2."}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"theory-explanation",children:"Theory Explanation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Robot Vision"})," (or ",(0,t.jsx)(n.strong,{children:"Computer Vision in Robotics"}),') is the field that enables robots to "see" and interpret their environment from image data. This is crucial for tasks like navigation, manipulation, object interaction, and human-robot collaboration.']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Camera Calibration"})," is the process of estimating the intrinsic and/or extrinsic parameters of a camera. Intrinsic parameters describe the camera's optical characteristics (focal length, principal point, lens distortion), while extrinsic parameters describe its position and orientation in a world coordinate system. Calibration is essential for accurate 3D reconstruction and measurements from 2D images."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Fundamental Computer Vision Algorithms"})," form the building blocks of robotic perception:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Image Filtering"}),": Techniques like Gaussian blur for noise reduction, Canny edge detection for contour extraction."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Detection"}),": Identifying salient points (e.g., corners, SIFT/SURF/ORB features) in an image that are robust to changes in viewpoint or lighting."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Segmentation"}),": Dividing an image into regions or objects (e.g., thresholding, color-based segmentation, more advanced deep learning methods)."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object Detection and Recognition"})," are key tasks:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Detection"}),": Locating instances of objects in an image and drawing bounding boxes around them (e.g., YOLO, SSD)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Recognition"}),": Identifying ",(0,t.jsx)(n.em,{children:"what"}),' an object is (e.g., classifying a detected object as a "cup" or "robot hand"). Traditional methods use feature descriptors and classifiers, while modern approaches heavily rely on deep learning (Convolutional Neural Networks - CNNs).']}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simultaneous Localization and Mapping (SLAM)"})," is a computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it. It's a cornerstone of mobile robotics, enabling robots to explore new areas and operate without prior maps. Visual SLAM uses camera data, often fused with IMU data (Visual-Inertial Odometry - VIO), to achieve this."]}),"\n",(0,t.jsx)(n.h4,{id:"diagrams",children:"Diagrams"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Raw Image Data] --\x3e B{Image Pre-processing};\n    B --\x3e C{Feature Extraction};\n    C --\x3e D{Object Detection/Recognition};\n    D --\x3e E[Semantic Understanding];\n\n    F[Camera Images] --\x3e G{Visual Odometry};\n    H[Sensor Data (IMU, LiDAR)] --\x3e I{Loop Closure Detection};\n    G & I --\x3e J[Map Optimization];\n    J --\x3e K[Robot Pose Estimation];\n    J --\x3e L[Environmental Map];\n    G -- Contributes to --\x3e J;\n    I -- Contributes to --\x3e J;\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px;\n    style B fill:#bbf,stroke:#333,stroke-width:2px;\n    style C fill:#bbf,stroke:#333,stroke-width:2px;\n    style D fill:#bbf,stroke:#333,stroke-width:2px;\n    style E fill:#f9f,stroke:#333,stroke-width:2px;\n    style F fill:#f9f,stroke:#333,stroke-width:2px;\n    style G fill:#bbf,stroke:#333,stroke-width:2px;\n    style H fill:#f9f,stroke:#333,stroke-width:2px;\n    style I fill:#bbf,stroke:#333,stroke-width:2px;\n    style J fill:#bbf,stroke:#333,stroke-width:2px;\n    style K fill:#f9f,stroke:#333,stroke-width:2px;\n    style L fill:#f9f,stroke:#333,stroke-width:2px;\n\n    subgraph Robot Perception Pipeline\n        B --\x3e C --\x3e D --\x3e E\n    end\n\n    subgraph SLAM Process\n        G & I & J & K & L\n    end\n\n    E -- Provides input to --\x3e L;\n    K -- Updates --\x3e G;\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Figure 7.1: Robot Perception and SLAM Overview"})}),"\n",(0,t.jsx)(n.h4,{id:"pythonros2-code-examples",children:"Python/ROS2 Code Examples"}),"\n",(0,t.jsx)(n.h5,{id:"python-with-opencv-edge-detection",children:"Python with OpenCV: Edge Detection"}),"\n",(0,t.jsxs)(n.p,{children:["This example uses OpenCV to perform Canny edge detection on an image. For this to run, you would need an image file (e.g., ",(0,t.jsx)(n.code,{children:"image.jpg"}),") in the same directory."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\ndef apply_canny_edge_detection(image_path):\n    # Read the image in grayscale\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    if img is None:\n        print(f\"Error: Could not open or find the image at {image_path}\")\n        return\n\n    # Apply Gaussian blur to reduce noise and help edge detection\n    blurred_img = cv2.GaussianBlur(img, (5, 5), 0)\n\n    # Apply Canny edge detection\n    # The two threshold arguments are for hysteresis procedure\n    # Recommended ratio between upper and lower threshold is 2:1 or 3:1\n    edges = cv2.Canny(blurred_img, 50, 150)\n\n    # Display original and edge-detected images\n    cv2.imshow('Original Image', img)\n    cv2.imshow('Canny Edges', edges)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\nif __name__ == '__main__':\n    # Replace 'image.jpg' with the path to your image file\n    apply_canny_edge_detection('image.jpg')\n"})}),"\n",(0,t.jsx)(n.h5,{id:"ros2-conceptual-vision-node-pseudocode",children:"ROS2: Conceptual Vision Node (Pseudocode)"}),"\n",(0,t.jsx)(n.p,{children:"This conceptual ROS2 node subscribes to a camera image topic, processes it (e.g., applies edge detection), and publishes the processed image to another topic."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# vision_processing_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge # ROS2 package to convert between ROS Image messages and OpenCV images\nimport cv2\nimport numpy as np\n\nclass VisionProcessingNode(Node):\n    def __init__(self):\n        super().__init__('vision_processing_node')\n        self.subscription = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n        self.publisher_ = self.create_publisher(\n            Image, 'camera/image_processed', 10)\n        self.bridge = CvBridge()\n        self.get_logger().info('VisionProcessingNode started.')\n\n    def image_callback(self, msg):\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        except CvBridgeError as e:\n            self.get_logger().error(f'CvBridge Error: {e}')\n            return\n\n        # --- Image Processing (Example: Canny Edge Detection) ---\n        gray_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n        blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n        edges = cv2.Canny(blurred_image, 50, 150)\n\n        # Convert processed OpenCV image back to ROS Image message\n        try:\n            processed_msg = self.bridge.cv2_to_imgmsg(edges, encoding='mono8')\n            processed_msg.header = msg.header # Maintain original timestamp and frame_id\n            self.publisher_.publish(processed_msg)\n            self.get_logger().info('Published processed image with edges.')\n        except CvBridgeError as e:\n            self.get_logger().error(f'CvBridge Error: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionProcessingNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h4,{id:"exercises--mcqs",children:"Exercises + MCQs"}),"\n",(0,t.jsx)(n.h5,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Camera Calibration Purpose"}),": Explain why camera calibration is a critical step before performing 3D measurements or advanced perception tasks with a robot vision system."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection vs. Recognition"}),": Describe a robotic scenario where object ",(0,t.jsx)(n.em,{children:"detection"})," is sufficient, and another where full object ",(0,t.jsx)(n.em,{children:"recognition"})," is necessary. Provide an example for each."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SLAM Benefits"}),": Discuss how SLAM enables a mobile robot to operate in an unknown environment. What are the key challenges in implementing a robust SLAM system?"]}),"\n"]}),"\n",(0,t.jsx)(n.h5,{id:"multiple-choice-questions",children:"Multiple Choice Questions"}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsx)(n.p,{children:"Which of the following is primarily concerned with estimating a camera's focal length and lens distortion?"}),(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Extrinsic Calibration"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Intrinsic Calibration"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Image Segmentation"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Object Tracking"]}),"\n"]})]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsx)(n.p,{children:"What is the main goal of SLAM in robotics?"}),(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","To perform real-time object classification."]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","To generate photorealistic 3D models of objects."]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","To simultaneously build a map of an unknown environment and localize the robot within it."]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","To control the robot's joint movements precisely."]}),"\n"]})]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.p,{children:["OpenCV's ",(0,t.jsx)(n.code,{children:"cv2.Canny()"})," function is typically used for:"]}),(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Image resizing."]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Color conversion."]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Edge detection."]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Noise reduction with blurring."]}),"\n"]})]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var s=i(6540);const t={},o=s.createContext(t);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);